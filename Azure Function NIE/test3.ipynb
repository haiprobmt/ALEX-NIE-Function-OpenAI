{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import openai\n",
    "import azure.functions as func\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import json\n",
    "from azure.cosmos import CosmosClient\n",
    "import hashlib\n",
    "from azure.keyvault.secrets import SecretClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these values with your Azure Key Vault URL\n",
    "keyvault_url = \"https://kv-nie.vault.azure.net/\"\n",
    "\n",
    "# Create a SecretClient using the DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "client_kv = SecretClient(vault_url=keyvault_url, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your own values, either in environment variables or directly here\n",
    "AZURE_STORAGE_ACCOUNT = client_kv.get_secret(\"azure-storage-account\").value\n",
    "AZURE_SEARCH_SERVICE = client_kv.get_secret(\"azure-search-service\").value\n",
    "AZURE_SEARCH_INDEX = client_kv.get_secret(\"azure-search-index\").value\n",
    "AZURE_OPENAI_SERVICE = client_kv.get_secret(\"azure-openai-service\").value\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = client_kv.get_secret(\"azure-openai-chatgpt\").value\n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_EMB_DEPLOYMENT\") or \"embedding\"\n",
    "AZURE_SEARCH_API_KEY = client_kv.get_secret(\"azure-search-api-key\").value\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\") or \"davinci\"\n",
    "\n",
    "KB_FIELDS_CONTENT = os.environ.get(\"KB_FIELDS_CONTENT\") or \"content\"\n",
    "KB_FIELDS_CATEGORY = os.environ.get(\"KB_FIELDS_CATEGORY\") or \"category\"\n",
    "KB_FIELDS_SOURCEPAGE = os.environ.get(\"KB_FIELDS_SOURCEPAGE\") or \"sourcepage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = AzureKeyCredential(AZURE_SEARCH_API_KEY)\n",
    "\n",
    "# Used by the OpenAI SDK\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "# Comment these two lines out if using keys, set your API key in the OPENAI_API_KEY environment variable and set openai.api_type = \"azure\" instead\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = \"4657af893faf48e5bd81208d9f87f271\"\n",
    "\n",
    "# Set up clients for Cognitive Search and Storage\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_system_message = \"\"\"You are a faculty who assists teachers design a course outline for their students. \n",
    "Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. \n",
    "Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"{base_system_message.strip()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-4-32k\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(messages, model_name, max_response_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=AZURE_OPENAI_CHATGPT_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "engage = \"\"\"\n",
    "        Please help to Explain constructive alignment. Describe and explain what is a blended course Describe and explain what are modes of learning (face to face, synchronous and/or asynchronous). \n",
    "        Explain Intended Learning Outcomes(ILO) that uses the ABCD (Audience, Behaviour, Condition, Degree) model in relation to the Bloom’s Taxonomy for the ‘behaviour’ component. \n",
    "        Describe and explain the verbs used for \"Behaviour\" component. Provide an example and identify which are the ABCD components. \n",
    "        Explain how more than one ILO can be achieved in one lesson in a course. \n",
    "        List a variety of both online and face to face learner – centred Teaching and Learning Activities (TLA). List a variety of corresponding Assessment Tasks (AT) which are either formative or summative. \n",
    "        Describe what it means to weave the TLAs from lesson to lesson. \n",
    "        List a variety of ed-tech (e.g. padlet, kahoot) tools to complement 3 modes of learner-centred TLAs. List a variety of other resources such as articles, books, videos to complement 3 TLAs. \n",
    "        Explain what should be included in writing a course synopsis.\n",
    "        Explain and analyze what is TE21 Framework and Components of V3SK . \n",
    "        Be detailed in your answers. Answer by adapting the frameworks listed in the list of sources below.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_category = None\n",
    "query_vector = openai.Embedding.create(engine=AZURE_OPENAI_EMB_DEPLOYMENT, input=engage)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Alternatively simply use search_client.search(q, top=3) if not using semantic search\n",
    "filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
    "r = search_client.search(\n",
    "                        engage, \n",
    "                        filter=filter,\n",
    "                        query_type=QueryType.SEMANTIC, \n",
    "                        query_language=\"en-us\", \n",
    "                        query_speller=\"lexicon\", \n",
    "                        semantic_configuration_name=\"default\", \n",
    "                        # top=3,\n",
    "                        vector=query_vector if query_vector else None, \n",
    "                        top_k=50 if query_vector else None,\n",
    "                        vector_fields=\"embedding\" if query_vector else None\n",
    "                        )\n",
    "results = results = [str(doc[KB_FIELDS_SOURCEPAGE]) + \": \" + str(doc[KB_FIELDS_CONTENT]).replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "content = \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "user_message = engage + \" \\nSOURCES:\\n\" + content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 156180\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 16384 tokens. However, your messages resulted in 156181 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[337], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m max_response_tokens \u001b[39m=\u001b[39m \u001b[39m2048\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[39m=\u001b[39m send_message(messages, AZURE_OPENAI_CHATGPT_DEPLOYMENT, max_response_tokens)\n\u001b[0;32m      4\u001b[0m messages\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: response})\n",
      "Cell \u001b[1;32mIn[331], line 4\u001b[0m, in \u001b[0;36msend_message\u001b[1;34m(messages, model_name, max_response_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_message\u001b[39m(messages, model_name, max_response_tokens\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      5\u001b[0m         engine\u001b[39m=\u001b[39;49mAZURE_OPENAI_CHATGPT_DEPLOYMENT,\n\u001b[0;32m      6\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[0;32m      7\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_response_tokens,\n\u001b[0;32m      9\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - deeeplabs.com\\Desktop\\NIE project\\Azure Function NIE\\.venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - deeeplabs.com\\Desktop\\NIE project\\Azure Function NIE\\.venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - deeeplabs.com\\Desktop\\NIE project\\Azure Function NIE\\.venv\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - deeeplabs.com\\Desktop\\NIE project\\Azure Function NIE\\.venv\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - deeeplabs.com\\Desktop\\NIE project\\Azure Function NIE\\.venv\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 16384 tokens. However, your messages resulted in 156181 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "\n",
    "max_response_tokens = 2048\n",
    "\n",
    "response = send_message(messages, AZURE_OPENAI_CHATGPT_DEPLOYMENT, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = \"a 5-week course outline on Food Fiction: A Creative Writing and Storytelling\"\n",
    "explore = \"\"\"\n",
    "            Thanks for your response, then with the the given explanations, create “{Input}” provide a course synopsis for the entire course within 200 words.\n",
    "            Write a maximum of 5 ILOs,using the ABCD model, for the entire \n",
    "            course.\n",
    "            For each week, list the topic and mode of delivery determined by \n",
    "            the nature of the content and TLAs. \n",
    "            For each week, provide a variety of TLAs and a variety of \n",
    "            formative assessment tasks (ATs) for the first 4 weeks and a \n",
    "            summative assessment task for the last week. Ensure the ATs \n",
    "            and TLAs are weaved.\n",
    "            For each week, alllow for more than one ILO to be achieved and \n",
    "            state which ILOs has been achieved in each lesson.\n",
    "            Suggest ed tech tools to support the TLAs in each week.\n",
    "            Suggest other learning resources (books,websites,journals) to \n",
    "            support the TLAs in each week\".\n",
    "            \"\"\".format(Input = Input)\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"role\": \"user\", \"content\": explore})\n",
    "\n",
    "max_response_tokens=2048\n",
    "\n",
    "response = send_message(messages, AZURE_OPENAI_CHATGPT_DEPLOYMENT, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain = \"\"\"\n",
    "            Greate! and for the course outline you just created, Examine if the ATs and TLAs support \n",
    "            the constructive alignment with the \n",
    "            ILOs.\n",
    "            Explain how are the various ILOs \n",
    "            achieved for each lesson\n",
    "            Examine if the ATs and TLAs from \n",
    "            week to week are weaved.\n",
    "            Explain how the nature of the content \n",
    "            and TLAs determine the mode of \n",
    "            delivery for each lesson. \n",
    "            Making sure, you provide your \n",
    "            explanation with reference to the \n",
    "            above course outline.\"\"\"\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"role\": \"user\", \"content\": explain})\n",
    "\n",
    "max_response_tokens=2048\n",
    "\n",
    "response = send_message(messages, AZURE_OPENAI_CHATGPT_DEPLOYMENT, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaborate = \"\"\"\n",
    "            Excellent! based on the above explaintions and the course you just created, present the \n",
    "            following information using a json format with this structure\n",
    "            {{\n",
    "                \"Course synopsis\": Information of Course synopsis,\n",
    "                \"ILOs for entire course\": List out maximum of 5 ILOs using the ABCD model, starting with Student will be able to... for each point,\n",
    "                \"TE21\": Analyze the keywords from Course synopsis and ILOs then generate TE21 information with skills, values and knowledge applying the components of V3SK\n",
    "                \"Week\": [\n",
    "                    \"Week\": number of this week,\n",
    "                    \"Course topics\": Information of Course topics in this week,\n",
    "                    \"ILOs achieved\": Which point number of ILOs achieved in this week,\n",
    "                    \"ATs\": Information of ATs in this week,\n",
    "                    \"TLAs\": Information of TLAs in this week, The TLAs must be explained clearly, exhibiting weaving from lesson to lesson\n",
    "                    \"Ed tech tools\" : Information of Ed tech tools in this week,\n",
    "                    \"Other learning resources\" : Information of learning resources in this week\n",
    "                    \"Mode of learning\": Information of Mode of learning in this week, \n",
    "                        Do take note that 30-60 percent of the course should be online and the nature of the lesson should decide the mode of learning\n",
    "                        just list out the suitable Mode of learning without percentage of each\n",
    "                    ]      \n",
    "            }} \n",
    "            \"\"\".format(Input = Input)\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"role\": \"user\", \"content\": elaborate})\n",
    "\n",
    "max_response_tokens=2048\n",
    "\n",
    "response = send_message(messages, AZURE_OPENAI_CHATGPT_DEPLOYMENT, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_reponse = len(messages)\n",
    "for i in range(0, len(messages) + 1):\n",
    "    if i == last_reponse:     \n",
    "        print(messages[i-1]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
